{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as cp\n",
    "import numpy as np\n",
    "\n",
    "X, y = cp.load(open('winequality-white.pickle','rb'))\n",
    "\n",
    "N, num_features = X.shape\n",
    "N_train = int(0.8 * N)\n",
    "N_test = N - N_train\n",
    "X_train = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handin 1 : \n",
    "Make a bar chart showing the distribution of y values appearing in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 7 artists>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2614b4780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import itemfreq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "%matplotlib inline\n",
    "plt.bar(unique, counts, align='center', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Handin 2 : Report the mean squared error, i.e., the average of the squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive mse 0.776777\n",
      "Naive mse_test 0.813857\n"
     ]
    }
   ],
   "source": [
    "average = np.mean(y_train)\n",
    "mse = np.mean((y_train - average)**2)\n",
    "mse_test = np.mean((y_test- average)**2)\n",
    "print (\"Naive mse %f\"%mse)\n",
    "print (\"Naive mse_test %f\"%mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mean = [] \n",
    "feature_std = [] \n",
    "for col in range(num_features):\n",
    "    feature_val = X_train[:,col]\n",
    "    mean = np.mean(feature_val)\n",
    "    std = np.std(feature_val)\n",
    "    std_feature = (feature_val - mean)/std \n",
    "    feature_mean.append(mean)\n",
    "    feature_std.append(std)\n",
    "    X_train[:,col] = std_feature \n",
    "    test_feature_val = X_test[:,col]\n",
    "    std_test = (test_feature_val - mean)/std \n",
    "    X_test[:,col] = std_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed-form Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training data : 0.564000\n",
      "MSE for test data : 0.560729\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "ones = np.ones(len(X_train))\n",
    "X_train_appended = np.column_stack((ones,X_train))\n",
    "X_transpose = np.transpose(X_train_appended)\n",
    "w = np.dot(np.dot(inv(np.dot(X_transpose, X_train_appended)),X_transpose),y_train)\n",
    "y_predict = np.dot(X_train_appended,w)\n",
    "mse = np.average((y_train - y_predict)**2)\n",
    "print (\"MSE for training data : %f\"%mse)\n",
    "X_test_appended = np.column_stack((np.ones(len(X_test)),X_test))\n",
    "y_test_predict =  np.dot(X_test_appended,w)\n",
    "mse_test = np.average((y_test - y_test_predict)**2)\n",
    "print (\"MSE for test data : %f\"%mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Checking with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for training data: 0.56\n",
      "Mean squared error for test data: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "#print('Coefficients: \\n', regr.coef_)\n",
    "predict = regr.predict(X_train)\n",
    "print(\"Mean squared error for training data: %.2f\"\n",
    "      % mean_squared_error(y_train,predict ))\n",
    "predict_test = regr.predict(X_test)\n",
    "print(\"Mean squared error for test data: %.2f\"\n",
    "      % mean_squared_error(y_test,predict_test ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Handin 4: Report the learning curves plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ecf79a9c0953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtesting_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = 20 \n",
    "max_datasets = 600\n",
    "datapoints = []\n",
    "training_error = []\n",
    "testing_error = [] \n",
    "\n",
    "while (datasets <600): \n",
    "    X_train_batch = X_train_appended[:datasets,:]\n",
    "    y_train_batch = y_train[:datasets]\n",
    "    X_transpose = np.transpose(X_train_batch)\n",
    "    w = np.dot(np.dot(inv(np.dot(X_transpose, X_train_batch)),X_transpose),y_train_batch)\n",
    "    y_predict = np.dot(X_train_batch,w)\n",
    "    mse = np.average((y_train_batch - y_predict)**2)\n",
    "    training_error.append(mse)\n",
    "    X_test = X_test_appended\n",
    "    y_test_predict =  np.dot(X_test,w)\n",
    "    mse_test = np.average((y_test - y_test_predict)**2)\n",
    "    testing_error.append(mse_test)\n",
    "    datasets = datasets + 20 \n",
    "    datapoints.append(datasets)\n",
    "    \n",
    "%matplotlib inline\n",
    "plt.plot(datapoints,training_error)\n",
    "plt.plot(datapoints,testing_error)\n",
    "\n",
    "plt.legend(['Training Error', 'Testing Error'], loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the model is not underfitting. After getting around 250 data points the training error and testing error plateaus and that's the amount of data needed to get the optimal test error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with poly degree 2\n",
      "mse for ridge regression with optimal lambda 1.000000 : 0.512589\n",
      "mse for lasso regression with optimal lambda 0.010000 : 0.527511\n",
      "working with poly degree 3\n",
      "mse for ridge regression with optimal lambda 100.000000 : 0.599301\n",
      "mse for lasso regression with optimal lambda 0.010000 : 0.514302\n",
      "working with poly degree 4\n",
      "mse for ridge regression with optimal lambda 100.000000 : 1.087639\n",
      "mse for lasso regression with optimal lambda 0.010000 : 0.521606\n",
      "working with poly degree 5\n",
      "mse for ridge regression with optimal lambda 100.000000 : 1.597427\n",
      "mse for lasso regression with optimal lambda 0.010000 : 0.508657\n",
      "optimal for ridge is degre 2 with mse 0.512589\n",
      "optimal for lasso is degre 5 with mse 0.508657\n"
     ]
    }
   ],
   "source": [
    "lambda_list = [0.01,0.1,1,10,100 ]\n",
    "\n",
    "X_train_new = X[:int(0.6*N)]\n",
    "X_validation = X[int(0.6*N):int(0.8*N)]\n",
    "y_train_new = y[:int(0.6*N)]\n",
    "y_validation = y[int(0.6*N):int(0.8*N)]\n",
    "X_train = X[:int(0.8*N)]\n",
    "y_train = y[:int(0.8*N)]\n",
    "X_test = X[int(0.8*N):]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "optimal_ridges = [] \n",
    "optimal_lassos = []\n",
    "poly_degrees = [2,3,4,5]\n",
    "for poly_degree in poly_degrees: \n",
    "    print ('working with poly degree %d'%poly_degree)\n",
    "    poly = PolynomialFeatures(poly_degree)\n",
    "    X_train_new_poly = poly.fit_transform(X_train_new)\n",
    "    X_validation_poly = poly.transform(X_validation)\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_transform = scaler.fit_transform(X_train_new_poly)\n",
    "    X_validation_transform = scaler.transform(X_validation_poly)\n",
    "    X_train_transform = scaler.transform(X_train_poly)\n",
    "    X_test_transform = scaler.transform(X_test_poly)\n",
    "\n",
    "    coefs_ridge = []\n",
    "    ridge_mse_list = [] \n",
    "    for lam in lambda_list : \n",
    "        ridge = Ridge(alpha=lam)\n",
    "        ridge.fit(X_transform, y_train_new) \n",
    "        y_validation_predict = ridge.predict(X_validation_transform)\n",
    "        coefs_ridge.append(ridge.coef_)\n",
    "        mse = mean_squared_error(y_validation,y_validation_predict)\n",
    "        ridge_mse_list.append(mse)\n",
    "     #   print ('lambda %f with mse %f '%(lam, mse))\n",
    "\n",
    "    optimal_lam_ridge = lambda_list[ridge_mse_list.index(min(ridge_mse_list))]\n",
    "\n",
    "    coefs_lasso = []\n",
    "    lasso_mse_list = [] \n",
    "    for lam in lambda_list : \n",
    "        lasso = Lasso(alpha=lam)\n",
    "        lasso.fit(X_transform, y_train_new) \n",
    "        y_validation_predict = lasso.predict(X_validation_transform)\n",
    "        coefs_lasso.append(lasso.coef_)\n",
    "        mse = mean_squared_error(y_validation,y_validation_predict)\n",
    "        lasso_mse_list.append(mse)\n",
    "    #    print ('lambda %f with mse %f '%(lam, mse))\n",
    "\n",
    "    optimal_lam_lasso = lambda_list[lasso_mse_list.index(min(lasso_mse_list))]\n",
    "\n",
    "\n",
    "    # running with optimal lambda for ridge \n",
    "    ridge = Ridge(alpha=optimal_lam_ridge)\n",
    "    ridge.fit(X_train_transform, y_train) \n",
    "    y_test_predict = ridge.predict(X_test_transform)\n",
    "    mse = mean_squared_error(y_test,y_test_predict)\n",
    "    optimal_ridges.append(mse)\n",
    "    print ('mse for ridge regression with optimal lambda %f : %f'%(optimal_lam_ridge,mse))\n",
    "\n",
    "    # running with optimal lambda for lasso \n",
    "    lasso = Lasso(alpha=optimal_lam_lasso)\n",
    "    lasso.fit(X_train_transform, y_train) \n",
    "    y_test_predict = lasso.predict(X_test_transform)\n",
    "    mse = mean_squared_error(y_test,y_test_predict)\n",
    "    optimal_lassos.append(mse)\n",
    "    print ('mse for lasso regression with optimal lambda %f : %f'%(optimal_lam_lasso,mse))\n",
    "\n",
    "\n",
    "print ('optimal for ridge is degre %d with mse %f'%(poly_degrees[optimal_ridges.index(min(optimal_ridges))], min(optimal_ridges)))\n",
    "print ('optimal for lasso is degre %d with mse %f'%(poly_degrees[optimal_lassos.index(min(optimal_lassos))],min(optimal_lassos)))\n",
    "# Plotting ... \n",
    "#    %matplotlib inline\n",
    "#    ax = plt.gca()\n",
    "\n",
    "#    ax.plot(lambda_list, coefs_ridge)\n",
    "#    ax.set_xscale('log')\n",
    "#    ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "#    plt.xlabel('alpha')\n",
    "#    plt.ylabel('weights')\n",
    "#    plt.title('Ridge coefficients as a function of the regularization')\n",
    "#    plt.axis('tight')\n",
    "#    plt.show()\n",
    "\n",
    "#    %matplotlib inline\n",
    "#    ax2 = plt.gca()\n",
    "#    ax2.plot(lambda_list, coefs_lasso)\n",
    "#    ax2.set_xscale('log')\n",
    "#    ax2.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "#    plt.xlabel('alpha')\n",
    "#    plt.ylabel('weights')\n",
    "#    plt.title('Lasso coefficients as a function of the regularization')\n",
    "#    plt.axis('tight')\n",
    "#    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to the Results, per the combinations tried, the optimal configuration for ridge regressino is degree 2 with lambda 1 and the optimal configuration for lasso is degree 5 with lambda 0.01"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
